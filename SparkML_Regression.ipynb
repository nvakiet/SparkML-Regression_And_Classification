{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkML_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1oBU1C3ZyyiElZMFhC35AArolp8WGrGMW",
      "authorship_tag": "ABX9TyMDKGQ8jLJfXZGgKe77xGeA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvakiet/SparkML-Regression_And_Classification/blob/main/SparkML_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Spark on Google Colab"
      ],
      "metadata": {
        "id": "A_G9F3Cg2T4T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tB3Vtt231pq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f914e47-a256-45ff-f2b7-f389a6a251e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the \"Predict Future Sales\" dataset from Kaggle"
      ],
      "metadata": {
        "id": "UgqVKmwOqnQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Kaggle python package and create directory for kaggle credential\n",
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "# Copy the Kaggle credential to runtime machine (remember to mount the Google Drive containing the credential file)\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the dataset and unzip\n",
        "!kaggle competitions download -c competitive-data-science-predict-future-sales\n",
        "!unzip competitive-data-science-predict-future-sales.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88CTFg1eqv21",
        "outputId": "bf36aed2-afb7-4d35-9ff3-a471d2f22f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "competitive-data-science-predict-future-sales.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  competitive-data-science-predict-future-sales.zip\n",
            "replace item_categories.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages and initialize a Spark session"
      ],
      "metadata": {
        "id": "jNxbpjLT2ZMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import *\n",
        "from pyspark.ml.evaluation import *"
      ],
      "metadata": {
        "id": "gWNQEVaf2elp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = (SparkSession\n",
        "        .builder\n",
        "        .appName('Lab 3 - SparkML: Regression')\n",
        "        .getOrCreate()\n",
        ")"
      ],
      "metadata": {
        "id": "vdGYbMh1qhE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load train and test datasets into Pyspark"
      ],
      "metadata": {
        "id": "yvQHktrs3Cra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = spark.read.csv(\"sales_train.csv\", header = True, inferSchema = True)\n",
        "test = spark.read.csv(\"test.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "xIlyH-tBPGDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raw data overview\n",
        "Firstly, let's see what does the raw data look like."
      ],
      "metadata": {
        "id": "5nmhs05pc39_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the dataframe schema\n",
        "print(\"Training data schema\")\n",
        "train.printSchema()\n",
        "print(\"Test data schema\")\n",
        "test.printSchema()"
      ],
      "metadata": {
        "id": "cxW10gAMojsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the first 10 samples\n",
        "train.show(10)"
      ],
      "metadata": {
        "id": "_iDacPV6FWne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate a summary of the whole dataframe to have a better understanding of the problem"
      ],
      "metadata": {
        "id": "CbY-GVPvrBqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Column number:\", len(train.columns))\n",
        "print(\"Raw data summary:\")\n",
        "train.summary().show()"
      ],
      "metadata": {
        "id": "vhlNEeTyq-7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the summary, we can see the dataset has a shape of (2935849,6). This is a time series where the item price and sales count depend on the dates. There's also the fact that the problem requires us to predict the sales count by month while this dataset is indexed by date, so some aggregation must be done on this dataframe.  \n",
        "Judging from the summary (mean, std, min, max) of item_price and item_cnt_day, there're definitely outliers in the data, and maybe some missing values.  \n",
        "Because the basic Spark MLlib library doesn't have any class for time series modeling, we must perform some feature engineering to convert this dataset from a time series forecasting problem to a regression problem so we can fit it with SparkML's regressors."
      ],
      "metadata": {
        "id": "RgqU6n_qsYes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing\n",
        "To prevent data leakage problem, we remove any item in training set which doesn't exist in test set. With Spark, this can be done by doing a right outer join."
      ],
      "metadata": {
        "id": "0J6EZQWazIKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform right outer join\n",
        "train = (train.join(test, [train.shop_id == test.shop_id, train.item_id == test.item_id], \"rightouter\")\n",
        "              .select(train.columns))\n",
        "# See if the number of rows decreases\n",
        "train.count()"
      ],
      "metadata": {
        "id": "k3YaR3C3ze3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "ukz_Pl0rzauu"
      }
    }
  ]
}